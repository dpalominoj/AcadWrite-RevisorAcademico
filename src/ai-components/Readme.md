##  Configuraci贸n de Ollama en el contenedor

Dentro de la carpeta `ia/ollama` se encuentra la instalaci贸n local de **Ollama** y los modelos descargados (`llama3:latest`). Esto permite ejecutar los nodos de IA de manera **offline** dentro del contenedor, sin necesidad de depender de una instalaci贸n externa o API externa.  

### Notas importantes
- La carpeta incluye:
  - Ejecutable de Ollama.
  - Modelos predescargados (por ejemplo `llama3:latest`).
  - Archivos de configuraci贸n internos.
- Aseg煤rate de mantener esta carpeta **dentro del contenedor** y no modificar la estructura, ya que los nodos en n8n apuntan a esta ubicaci贸n.
- Si necesitas actualizar los modelos, puedes usar el comando correspondiente de Ollama dentro del contenedor:
  ```bash
  ollama pull llama3:latest
